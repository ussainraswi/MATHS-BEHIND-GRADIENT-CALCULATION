# MATHS-BEHIND-GRADIENT-CALCULATION
Derivative of a loss function w.r.t its parameter is known as Grad function.

 - Using the following example, we change the loss function to two times
the differences between the input and the output tensors, instead of
MSELoss function. The following grad_fn, which is defined as a custom
function, shows the user how the final output retrieves the derivative of
the loss function.
# grad_fn:-
is a derivative of the loss function with respect to the parameters of the model.
This is exactly what we do in the following grad_fn.


THANKS & REGARDS,
USSAIN RASWI
